{"cells":[{"cell_type":"code","source":["import logging\nlogging.getLogger(\"py4j\").setLevel(logging.INFO)\nlogging.getLogger('pyspark').setLevel(logging.ERROR)\nlogger = logging.getLogger('pyspark')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a24b3283-1b47-4e0b-9c42-f1e3c874e028"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["!pip install mlflow"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4a75f0df-ee0e-4437-881b-5b82a7e89f1e"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["!pip install --upgrade pip\n!pip install transformers==4.12.5\n!pip install simpletransformers==0.63.3\n!pip install tensorboardX==2.4\n#!pip install torch==1.7.1+cu110\n!pip install tensorflow==2.6.2"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"33cab6d7-4554-448e-b0f5-4e4b97e2bb12"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["!pip install torch==1.7.1"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"90ddb107-f588-46c7-962e-65bc0be6cd37"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def get_dir_content(ls_path):\n    dir_paths = dbutils.fs.ls(ls_path)\n    return [ [p.path.replace('dbfs:','/dbfs'), os.path.splitext(p.path)[1].replace('.','')] \n                for p in dir_paths \n                    if p.isFile()] \n\ndef exists(path):\n    try:\n        dbutils.fs.ls()\n        return True\n    except:\n        return False\n    \ndef createIfNotExists(path):\n    if (not exists(path)):\n        dbutils.fs.mkdirs(path)\n        \n## define path and mount to cluster\n## Обратить внимание:\n# 1. pointer_folder - следует формировать с учетом полного пути к папке с учетом родительских подпапок\n# 2. Если по пути монтирования уже есть папка, которая смонтирована с другим хранилищем, то сначала нужно отмонтировать старое хранилище. Например, к папке rawdata был примонтировано Blob Storage, затем эту же папку хотим примонтировать к Data Lake хранилищу.\ndef define_path_and_mount(container, staccount):\n    print(\"define_path_and_mount: container-{}, staccount-{}\".format(container, staccount))\n    sp_clientId = \"465f0038-39af-4f0c-9e40-8dbfbd99936f\"\n    sp_tenantId = \"72162faa-c4d3-4ed6-89bd-a37642170063\"\n    db_scope_name = \"scope-adept\"\n    db_keyvault_name = \"secret-adept-4-adls-databricks\"\n    db_endpoint = \"https://login.microsoftonline.com/{}/oauth2/token\".format(sp_tenantId)\n    \n    uri_adls = \"abfss://{}@{}.dfs.core.windows.net/\".format(container, staccount)    \n    pointer_folder = \"/mnt/{}/\".format(container)\n\n    configs = {\"fs.azure.account.auth.type\": \"OAuth\",\n              \"fs.azure.account.oauth.provider.type\": \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\",\n              \"fs.azure.account.oauth2.client.id\": sp_clientId,\n              \"fs.azure.account.oauth2.client.secret\": dbutils.secrets.get(scope=db_scope_name,key=db_keyvault_name),\n              \"fs.azure.account.oauth2.client.endpoint\": db_endpoint}\n    #print(configs)\n    # Optionally, you can add <directory-name> to the source URI of your mount point.\n    try:\n        dbutils.fs.mount(\n        source = uri_adls,\n        mount_point = pointer_folder,\n        extra_configs = configs)\n\n        print ('The mount point folder is mounted.')\n    except:\n        print ('The mount point folder is mounted yet.')\n\n    return pointer_folder\n\ndef put_log(url, msg_template, state, message):\n    msg = msg_template.replace(\"$state\", state)\n    msg = msg.replace(\"$message\", message)\n    print(\"put_log url:{}, message:{}\".format(url, msg))\n    params = {'message':msg}\n    try:\n        r = requests.post(url, params=params)\n        print(r.status_code, r.reason)\n    except Exception as ex:\n        print(ex)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d4735b44-3ed0-461e-ac02-37b1277504c5"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["container = \"adept\"\nstaccount = \"bruweadls001\"\nbase_folder = \"UDL/Internal Sources/Manual Files/Agreements/\"\nconverted_path = \"Converted/\"\nprocessed_path = \"Processed/\"\nkey_directory  = \"2021-12-13-12-49-23-437b2a97-41e7-430e-85e3-666e592b94c3\"\nurl_logging = 'https://fn-upload-file-to-adls.azurewebsites.net/api/QueueRequest?code=DEbXSIGQF1WT9HYB8epmymzw5USPFDK5/kbvi1ph4vbx9Ww60y6y2w==&command=put&key-dir=2021-12-13-12-49-23-437b2a97-41e7-430e-85e3-666e592b94c3'\nmsg_template = \"{\\\"state\\\": \\\"$state\\\",\\\"message\\\":\\\"$message\\\"}\"\nstate_outer = \"55\"\n\ntry:\n    container = dbutils.widgets.get(\"container\")\nexcept:\n    pass\ntry:\n    staccount = dbutils.widgets.get(\"storage\")\nexcept:\n    pass\ntry:\n    base_folder = dbutils.widgets.get(\"basePath\")\nexcept:\n    pass\ntry:\n    converted_path = dbutils.widgets.get(\"subPathConverted\")\nexcept:\n    pass\ntry:\n    processed_path = dbutils.widgets.get(\"subPathProcessed\")\nexcept:\n    pass\ntry:\n    key_directory = dbutils.widgets.get(\"keyDirectory\")\nexcept:\n    pass\ntry:\n    url_logging = dbutils.widgets.get(\"urlLogging\")\nexcept:\n    pass\ntry:\n    msg_template = dbutils.widgets.get(\"msgTemplate\")\nexcept:\n    pass\ntry:\n    state_outer = dbutils.widgets.get(\"stateOuter\")\nexcept:\n    pass"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f8baac23-c19a-4509-bf6c-634a02c395cc"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["import mlflow.tensorflow\nimport mlflow\nimport mlflow.sklearn\nfrom mlflow.tracking import MlflowClient\nimport os\nfrom simpletransformers.classification import ClassificationModel, ClassificationArgs\nimport pandas as pd\nimport numpy as np\nfrom sklearn.metrics import f1_score, accuracy_score, roc_auc_score\nimport requests\n#import my_utils\n\nprint(\"Main forecasting\")\nbase_path = define_path_and_mount(container, staccount)\nsrc_path = os.path.join(base_path, base_folder, converted_path, key_directory) \ndst_path = os.path.join(base_path, base_folder, processed_path, key_directory)\nprint(base_path)\nprint(src_path)\nprint(dst_path)\ncreateIfNotExists(dst_path)\n\n#model_test = ClassificationModel('bert', 'https://github.com/AwaraItSolution/ADBricks-MLFlows', use_cuda = False) # использование CPU\nmodel_test = ClassificationModel('bert', 'SvyatoslavA/model_awara_text', use_cuda = False) # использование GPU\n    "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"228f66af-dbf4-406b-b909-c6d0f7fc99cb"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["input_files = get_dir_content(src_path)\nprint(input_files)\nfor full_file_name, extension in input_files:\n    #print(full_file_name)\n    file_name = os.path.basename(full_file_name)\n    try:\n        if extension == 'csv':\n            df = pd.read_csv(full_file_name, quotechar='\"')\n            # удаляем строки, содержащие null в столбце 'text'\n            df.drop(index=df.loc[df['text'].isna()].index.tolist(), inplace=True)\n        \n            test_text = df['text'].values.tolist()\n            predictions = model_test.predict(test_text)\n            df['result'] = predictions[0]\n\n            path_out = '/dbfs' + os.path.join(dst_path, file_name)\n            df.to_csv(path_out, index=False, quotechar='\"')\n            \n            put_log(url_logging, msg_template, state_outer, \"Файл: {} прогноз выполнен\".format(file_name))\n    except Exception as ex:\n        print(ex)\n        put_log(url_logging, msg_template, state_outer, \"Ошибка прогнозирования файла {}: {}\".format(file_name, ex))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"38c65602-0299-4ac8-8bba-7b8c76e47ddf"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#print(src_path)\n#dbutils.fs.ls('/mnt/adept/UDL/Internal Sources/Manual Files/Agreements/Models/')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e0e9f273-814a-4f2c-b338-a597f3814200"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#d = {'point': ['1.1','1.2'], 'text': [\"Настоящие правила регулируют отношения между АО «СК «ПАРИ» (далее - Страховщик) и юридическими или дееспособными физическими лицами (далее - Страхователи) при страховании #воздушным, морским, речным грузов, перевозимых транспортом. автомобильным, железнодорожным, \",\n#                                      \"По договору страхования Страховщик обязуется за обусловленную договором страхования плату (страховую премию) при наступлении предусмотренного в договоре страхования события #(страхового случая) возместить Страхователю или иному лицу, в пользу которого заключен договор страхования события убытки в застрахованном грузе (выплатить страховое возмещение) в пределах обусловленной договором #страхования суммы (страховой суммы). (Выгодоприобретателю), причиненные вследствие этого \"],\n#     'result': ['',''], 'annotation': ['','']}\n\n#df = pd.DataFrame(data=d)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8e110ebb-b0c3-4e53-8a4f-6df4acf27db7"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#test_text = df['text'].values.tolist()\n#predictions = model_test.predict(test_text)\n#df['result'] = predictions[0]\n#path_out = os.path.join(data_path_out, filename)\n#print(df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9c498e23-077e-4fc7-9e6b-5b85770a4cdd"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#client = MlflowClient()\nimport './aiutils'\n\n#%run \"/Users/evgeny.popovich@awara-it.com/aiutils\"\n#%run \"./aiutils\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cf0cde84-876a-4a1c-98a9-74e15e4e3a04"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]},"transient":null},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"<div class=\"ansiout\"><span class=\"ansi-cyan-fg\">  File </span><span class=\"ansi-green-fg\">&#34;&lt;command-351178247168681&gt;&#34;</span><span class=\"ansi-cyan-fg\">, line </span><span class=\"ansi-green-fg\">2</span>\n<span class=\"ansi-red-fg\">    import &#39;./aiutils&#39;</span>\n           ^\n<span class=\"ansi-red-fg\">SyntaxError</span><span class=\"ansi-red-fg\">:</span> invalid syntax\n</div>","errorSummary":"<span class=\"ansi-red-fg\">SyntaxError</span><span class=\"ansi-red-fg\">:</span> invalid syntax","metadata":{},"errorTraceType":"html","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-cyan-fg\">  File </span><span class=\"ansi-green-fg\">&#34;&lt;command-351178247168681&gt;&#34;</span><span class=\"ansi-cyan-fg\">, line </span><span class=\"ansi-green-fg\">2</span>\n<span class=\"ansi-red-fg\">    import &#39;./aiutils&#39;</span>\n           ^\n<span class=\"ansi-red-fg\">SyntaxError</span><span class=\"ansi-red-fg\">:</span> invalid syntax\n</div>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["path = define_path_and_mount(container, staccount)\nprint(path)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"596e982a-466b-41a3-a957-7d6edadb94c0"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"adb-rubert-classification-predict","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":3966433136513645}},"nbformat":4,"nbformat_minor":0}
